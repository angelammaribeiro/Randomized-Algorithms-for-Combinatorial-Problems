\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
	T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

	itle{Randomized Algorithms for Maximum Weighted Matching in General Graphs\\
{\footnotesize Advanced Algorithms Course Project, 2024/2025}}

\author{\IEEEauthorblockN{\^Angela M. Ribeiro}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
		extit{University of Aveiro}\\
Aveiro, Portugal \\
angelammaribeiro@ua.pt}}

\maketitle
\begin{abstract}
A study of randomized heuristics for Maximum Weighted Matching (MWM) on general graphs was conducted. The Python codebase exposes three generators: pure random sampling, randomized greedy construction, and simulated-annealing local search, run by an adaptive controller that caps candidates, time, and stagnation. Every run is instrumented to report candidate, feasibility, and weight operations plus runtime, enabling hardware-independent comparisons across project, teacher, and public datasets. The collected data reveal how each heuristic balances speed and accuracy, including the largest Erd\H{o}s--R\'enyi graph solvable under tight budgets.
\end{abstract}

\begin{IEEEkeywords}
Maximum weighted matching, randomized algorithms, simulated annealing, greedy heuristics, performance analysis, scalability
\end{IEEEkeywords}

\section{Introduction}
Maximum Weighted Matching (MWM) seeks a set of disjoint edges with maximum total weight and underpins resource-allocation, scheduling, and network-design tasks \cite{Edmonds1965,Schrijver2003}. Classical exact methods such as Gabow's and Edmonds' blossom algorithm are cubic or worse in $|V|$, limiting their practicality on dense or noisy graphs \cite{Edmonds1965,Gabow1990,Duan2014}. Randomized heuristics trade guaranteed optimality for tunable runtime, exploiting stochastic exploration to cover more of the search space within a fixed budget \cite{KarpSipser1981,Kirkpatrick1983}.

This project implements three such heuristics---pure random sampling, randomized greedy construction, and simulated-annealing local search---driven by an adaptive controller that enforces limits on candidates, time, and stagnation. Instrumentation decouples algorithmic work from hardware by counting candidate generation, feasibility checks, and weight evaluations per run. The code, dataset loaders, and experiment scripts remain in this repository so that all measurements can be regenerated locally via \texttt{run\_all\_experiments.py}. Section~II explains the algorithms, Section~III the datasets and protocol, Section~IV the complexity analysis, Section~V the empirical results, Section~VI the scalability probe, and Section~VII the conclusions.

\section{Randomized Algorithm Design}

This section outlines the three generators plus the adaptive controller. All components share the \texttt{Graph} abstraction, a feasibility checker, and instrumentation hooks so that every sampled matching contributes consistent statistics.

\subsection{Graph Representation and Utilities}

Graphs are stored as edge lists indexed by unique \texttt{EdgeIndex} values plus a hash map for duplicate detection. The constructor validates vertex bounds, removes self-loops, and keeps only the heaviest copy of any repeated edge, mirroring the ingestion logic in \texttt{datasets.py}. Matchings are tuples of edge indices; canonicalization sorts them so duplicate samples can be skipped. Feasibility scans the chosen edges to ensure disjoint endpoints, and weight evaluation sums the corresponding edge weights in a single pass.

\subsection{Pure Random Matching Generator}

	exttt{generate\_random\_matching} shuffles the edge indices with \texttt{random.shuffle} and greedily accepts feasible edges until the list ends or a \texttt{target\_size} hint is met. The lack of weight bias yields diverse matchings with $O(m)$ work per candidate and minimal per-run state.

\subsection{Randomized Greedy Matching Generator}

	exttt{generate\_random\_greedy\_matching} perturbs each edge weight by $1 + r\epsilon$, sorts the scores, and runs a greedy pass. The bias mode (linear, quadratic, or softmax) and noise level are exposed via parameters, matching the options in \texttt{randomized\_mwm.py}. Sorting drives the $O(m \log m)$ cost per candidate but typically returns heavier matchings than pure sampling.

\subsection{Simulated-Annealing Local Search}

	exttt{local\_search\_matching} starts from an optional seed and iterates up to \texttt{max\_iterations} times. Each step attempts an add, swap, or removal move (probabilities $0.5$, $0.3$, $0.2$), accepting improvements immediately and worse moves with simulated-annealing probability $\exp(\Delta/T)$. Temperature decays by \texttt{cooling}, occasional restarts occur with \texttt{restart\_probability}, and the routine returns the best canonical matching observed.

\subsection{Adaptive Randomized Search Controller}

	exttt{run\_random\_search} repeatedly invokes the selected generator, canonicalizes the result, rejects duplicates, checks feasibility, and updates the best matching. Runtime is limited by \texttt{max\_candidates}, an optional \texttt{time\_limit}, or \texttt{max\_no\_improve}. A boolean \texttt{adapt\_size} option jitters the \texttt{target\_size} hint toward the current best. The controller records candidate, feasibility, and weight operations via \texttt{add\_operations} and returns comprehensive telemetry (counts, histograms, weight history, and stopping reason) that Section~III consumes.

\section{Datasets, Instrumentation, and Experimental Protocol}

This section summarizes the dataset loaders, instrumentation, and experiment scripts that exercise the randomized solvers.

\subsection{Dataset Sources and Format Support}

The \texttt{datasets} module centralizes loading for project folders, teacher archives, and public repositories. It detects edge lists, adjacency matrices, JSON, Matrix Market, TSPLIB, SNAP text, and gzip-compressed variants, converting all of them into \texttt{Graph} instances (Table~\ref{tab:formats}). Format detection relies on file extensions and lightweight header inspection, mirroring the logic in \texttt{datasets.py}.

\begin{table}[htbp]
\caption{Supported Graph Formats}
\begin{center}
\begin{tabular}{|l|l|l|}
\hline
	extbf{Format} & \textbf{Extension} & \textbf{Source} \\
\hline
Edge list & .txt, .dat & Project, teacher \\
Adjacency matrix & .txt & Project, teacher \\
JSON & .json & Project \\
Matrix Market & .mtx & UF Collection \cite{Davis2011} \\
TSPLIB & .txt & OR benchmarks \\
SNAP & .txt.gz & SNAP \cite{Leskovec2016} \\
\hline
\end{tabular}
\label{tab:formats}
\end{center}
\end{table}

	exttt{load\_graph\_directory} whitelists known extensions, skips README/license files, and keeps only the heaviest duplicate edge so parsed graphs match the core \texttt{Graph} policy. Public data such as SNAP \texttt{ca-GrQc} and \texttt{email-Enron} \cite{Leskovec2016} or Matrix Market files \cite{Davis2011} are downloaded to \texttt{data\_cache/} on demand, while Mendeley and Brunel sets \cite{Beasley1990} can be added manually. Experiments in this report rely on the $12$ project and teacher graphs (6--50 vertices, densities 0.2--0.8) plus synthetic Erd\H{o}s--R\'enyi graphs \cite{ErdosRenyi1959} for the scalability probe.

\subsection{Performance Instrumentation}

The \texttt{performance} module exposes lightweight counters for \texttt{candidate\_generation}, \texttt{feasibility\_check}, and \texttt{weight\_evaluation}, incremented inside \texttt{run\_random\_search}. Because these counts ignore interpreter overhead, they are portable across machines and align with the ``basic operation'' metric in the code. Supplementary helpers such as \texttt{time\_function}, \texttt{estimate\_time\_complexity}, and \texttt{empirical\_scaling\_curve} operate on aggregated $(n, T)$ or $(n, \text{ops})$ pairs to estimate scaling trends and produce optional plots.

\subsection{Accuracy Evaluation}

For graphs with up to $20$ edges, \texttt{brute\_force\_optimal} enumerates every feasible subset to obtain the exact reference weight. Larger graphs fall back to the best empirical weight across algorithms. \texttt{AccuracyMetrics} and \texttt{AccuracySummary} store absolute/relative gaps and approximation ratios, and \texttt{compare\_algorithms} packages these summaries per graph.

\subsection{Experimental Protocol}

	exttt{run\_benchmark\_suite} iterates over every graph-algorithm pair, repeats runs with deterministic seeds, and records the resulting \texttt{ExperimentResult} plus CSV and JSON artifacts. Optional flags trigger brute-force accuracy summaries or convergence plots. \texttt{find\_largest\_graph} builds increasingly large Erd\H{o}s--R\'enyi graphs with density $\min(0.5, 4/n)$, stopping once the per-graph time limit is exceeded and fitting empirical scaling curves to the collected vertex/runtime pairs. The CLI entrypoint \texttt{run\_all\_experiments.py} wires these pieces together, letting users specify dataset sources, algorithms, budgets, and output directories, with \texttt{find\_largest\_graph} exposing the scalability probe.

\section{Formal Complexity Analysis}

Let $G = (V, E)$ denote an input graph with $n = |V|$ vertices and $m = |E|$ edges. The randomized framework exposes three tunable limits: the maximum number of candidates $K$, the optional wall-clock time $T$, and the number of simulated-annealing iterations $L$. Our analysis expresses running times in terms of $n$, $m$, and $K$ (or $L$ when local search is invoked) and relates them to the basic-operation metric defined in Section~III. Each basic operation corresponds to one of three dominant phases in the controller loop---candidate generation, feasibility checking, and weight evaluation---and therefore captures the algorithmic work independently of processor speed or interpreter overhead.

\subsection{Randomized Generators}

	extbf{Pure random sampling:} \\
		exttt{generate\_random\_matching} shuffles the $m$ edge indices in $O(m)$ time, scans them once, and inserts each edge only if both endpoints remain free. The worst-case per-candidate cost is $O(m)$, reduced to $O(\min(m, n))$ when \texttt{target\_size} permits early exit.

	extbf{Randomized greedy construction:} \\
		exttt{generate\_random\_greedy\_matching} perturbs every edge weight, sorts the resulting scores in $O(m \log m)$ time, and performs a single greedy pass. Over $K$ candidates the total work is $O(K m \log m)$, matching the implementation's dominant operations.

	extbf{Simulated-annealing local search:} \\
		exttt{local\_search\_matching} executes $L$ moves, each touching at most $\Delta$ edges (bounded by local degree). The resulting $O(L \Delta)$ cost aligns with the add/swap/remove primitives in the code, while temperature and restart updates stay $O(1)$.

\subsection{Adaptive Controller}

The \texttt{run\_random\_search} controller repeatedly: (i) invokes a generator, (ii) canonicalizes the resulting matching $M$ by sorting its $|M|$ edge indices ($O(|M| \log |M|)$), (iii) performs an expected $O(1)$ duplicate check via hashing, (iv) validates feasibility in $O(|M|)$ time, and (v) evaluates the weight in another $O(|M|)$ pass. If $C_{\text{gen}}$ denotes the cost of the active generator, a run that produces $K'$ distinct candidates ($K' \le K$) requires
\begin{equation}
O\left( K' \cdot (C_{\text{gen}} + |M| \log |M|) \right) \subseteq O\left( K' \cdot (C_{\text{gen}} + n \log n) \right).
\end{equation}
Because $|M| \le n/2$, the $n \log n$ term rarely dominates $C_{\text{gen}}$. The optional \texttt{adapt\_size} heuristic perturbs the target size via constant-time arithmetic and therefore adds only $O(K')$ overhead. Feasibility rejections and duplicate hits short-circuit the loop earlier, lowering the effective $K'$ captured in the operation counts.

\subsection{Operation-Metric Consistency}

The instrumentation records exactly one counter increment for each execution of candidate generation, feasibility verification, and weight computation. Canonicalization and histogram updates contribute lower-order terms but do not alter the linear relationship between the number of successfully evaluated candidates and the total operation count. Consequently, the slopes fitted to $(n, \text{ops})$ pairs in Section~V provide an empirical estimate of the dominant term in the asymptotic expressions derived above. Deviations between observed slopes and theoretical predictions reveal either cache-level effects or stochastic variations in $K'$.

\subsection{Comparison with Exact Algorithms}


Classical exact MWM algorithms such as Edmonds' blossom method \cite{Edmonds1965}, Gabow's data-structure enhancements \cite{Gabow1990}, and the near-linear approximation scheme of Duan and Pettie \cite{Duan2014} run in $O(n^3)$, $O(n^3)$, and $O(m \log^2 n)$ time, respectively, while guaranteeing optimal or $(1 - \varepsilon)$-approximate matchings. The randomized heuristics analyzed above require only $O(m)$ to $O(m \log m)$ work per candidate and admit tight budget control through $K$, $T$, and $L$. When $K$ is chosen so that $K m \log m \ll n^3$, the heuristics deliver substantial speedups while still producing near-optimal matchings in practice. Section~V will quantify this trade-off by comparing measured operation counts against the formal bounds.

\section{Experimental Results}

All three randomized algorithms were evaluated under consistent resource limits so that comparisons remain fair across datasets. Every run invoked \texttt{run\_all\_experiments.py} with the \texttt{random}, \texttt{greedy}, and \texttt{local} generators spanning the twelve project graphs, the twelve teacher graphs, and the synthetic Erd\H{o}s--R\'enyi family described in Section~III. Each algorithm received $400$ candidate attempts, a $2$~s wall-clock limit, and controller instrumentation (a two-second budget approximates the per-instance latency a practitioner might tolerate while still leaving room for exploration). Because the algorithms are randomized, all values reported in this section aggregate the median of three repetitions to smooth out stochastic variance. The experiment campaign emitted more than 200 JSON artifacts plus accuracy summaries under the default \texttt{results/} output folder; these generated files were intentionally removed from version control, so readers should rerun \texttt{run\_all\_experiments.py} to recreate the metrics and figures referenced below on their own machines.

\subsection{Runtime, Operations, and Accuracy}

Table~\ref{tab:aggregate-metrics} condenses the median runtime and operation counts per algorithm alongside the mean approximation ratio over all graphs with an exact or empirical reference. Pure sampling and randomized greedy spend most of their budget on candidate generation ($\approx 2.2 \times 10^5$ operations/run), while local search performs only hundreds of generator calls because each simulated-annealing move reuses much of the incumbent structure. Despite comparable runtimes (bounded by the $2$~s limit), the extra structure in greedy and local search translates into noticeably higher approximation ratios.

\begin{table}[htbp]
\caption{Aggregate runtime/operation/accuracy metrics (median or mean across all graphs).}
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
Algorithm & Runtime (s) & Candidate ops & Feasibility ops & $\overline{\rho}$ \\
\hline
Random & $2.00$ & $2.25\times 10^5$ & $1.39\times 10^2$ & $0.9376$ \\
Greedy & $2.00$ & $2.18\times 10^5$ & $1.0\times 10^1$ & $0.9764$ \\
Local & $2.00$ & $8.94\times 10^2$ & $5.0$ & $0.9963$ \\
\hline
\end{tabular}
\label{tab:aggregate-metrics}
\end{center}
\end{table}

The accuracy gaps concentrate below $1\%$ for greedy and local search, whereas random sampling occasionally trails the best reference by up to $5.5\%$ (e.g., \texttt{graph\_n10\_d50}). Figure~\ref{fig:gap-boxplot} visualizes this spread by plotting the relative-gap distribution across all graphs.

\subsection{Gap Analysis and Convergence Behaviour}

Figure~\ref{fig:weight-trajectory} tracks the weight obtained after every accepted candidate on \texttt{graph\_n15\_d50}. Greedy achieves a strong solution within three iterations thanks to its biased sorting step, local search continues to polish the weight during the annealing schedule, and random sampling requires tens of improvements to match their quality. Combined with the box plot, these trajectories confirm that structured heuristics turn the same $2$~s budget into better matchings rather than more attempts.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/weight_trajectory_graph_n15_d50.pdf}
\caption{Weight progression on \texttt{graph\_n15\_d50}. Greedy converges in a handful of steps, local search refines the solution via annealing moves, and random sampling relies on many accepted candidates.}
\label{fig:weight-trajectory}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/relative_gap_distribution.pdf}
\caption{Distribution of relative gaps across all graphs. Greedy and local search remain within $1\%$ of the optimal/reference weight, while random occasionally drifts to $5.5\%$.}
\label{fig:gap-boxplot}
\end{figure}

\subsection{Per-Graph Winners}

Table~\ref{tab:graph-winners} summarizes representative graphs spanning the teacher set (\texttt{SW*}) and the denser synthetic benchmarks. Greedy wins $28/77$ graphs, local search $27/77$, and random sampling $22/77$, with local dominating on larger, denser inputs such as \texttt{SWlargeG} and \texttt{graph\_n20\_d50} thanks to its ability to escape plateaus (recall that \texttt{SWlargeG} exceeded the standard limit only because we lifted the cap intentionally). Random still prevails on the smallest dense instances where brute-force accuracy is easy to reach.

\begin{table}[htbp]
\caption{Representative per-graph winners (gap is relative to the optimal or best empirical baseline).}
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
Graph & Winner & Gap (\%) & Avg. runtime (s) \\
\hline
	exttt{SWtinyG} & Greedy & $0.00$ & $2.00$ \\
	exttt{SWmediumG} & Greedy & $0.55$ & $0.21$ \\
	exttt{SWmediumDG} & Local & $0.00$ & $2.00$ \\
	exttt{SWmediumEWD} & Greedy & $0.83$ & $0.21$ \\
	exttt{SWlargeG} & Local & $0.007$ & $687.05$ \\
	exttt{SW10000EWD} & Greedy & $0.11$ & $2.03$ \\
	exttt{graph\_n10\_d50} & Random & $5.41$ & $2.00$ \\
	exttt{graph\_n20\_d50} & Local & $1.44$ & $2.00$ \\
\hline
\end{tabular}
\label{tab:graph-winners}
\end{center}
\end{table}

\subsection{Scaling Trends}

The runtime-scaling plot in Figure~\ref{fig:runtime-scaling} focuses on density-$0.5$ graphs with $n \in [4, 20]$. Random sampling flattens first because it reaches the duplicate-candidate limit quickly, whereas greedy scales super-linearly due to repeated $O(m \log m)$ sorts. Local search remains flat until $n \approx 15$, after which the per-move bookkeeping cost grows modestly. These empirical slopes align with the theoretical bounds derived in Section~IV and confirm that tuning $K$ or $L$ is the dominant lever for trading runtime for accuracy.

In summary, randomized greedy offers the best day-to-day speed--accuracy compromise, simulated annealing dominates on the largest and densest graphs, and pure random sampling is competitive only on tiny instances where optimal matchings are easy to find.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/runtime_scaling_density_50.pdf}
\caption{Runtime scaling on density-$0.5$ synthetic graphs. All algorithms obey the $2$~s cap, but greedy grows due to repeated sorting while local search stays nearly flat.}
\label{fig:runtime-scaling}
\end{figure}

\section{Scalability Probe}
\subsection{Largest Graph Solvable Under Time Budget}
We invoked \texttt{find\_largest\_graph} to stress each heuristic on synthetic $G(n, p)$ instances with $p = \min(0.5, 4/n)$ so that the average degree stays near four while the density shrinks for large $n$. Under $p=4/n$, the expected number of edges is $\mathbb{E}[|E|] = p\binom{n}{2} \approx 2n$, keeping greedy's $m \log m$ term manageable even as $n$ grows because $m$ never exceeds a linear function of $n$. Every run inherits the same controller contract used in Section~V---$2$~s wall-clock budget, $2{,}000$ candidate attempts, stagnation detection, and the instrumentation counters for candidate generation, feasibility, and weight evaluations. We sweep $n$ in steps of two (random/local) or four (greedy) until a run exceeds the budget; the resulting traces (Fig.~\ref{fig:largest-graph-scaling}) and checkpoints (Table~\ref{tab:largest-graph}) summarize the largest graphs solvable before the controller times out.

...existing code...

\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Randomized Algorithms for Maximum Weighted Matching in General Graphs\\
{\footnotesize Advanced Algorithms Course Project, 2024/2025}}

\author{\IEEEauthorblockN{Ângela M. Ribeiro}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
    \textit{University of Aveiro}\\
Aveiro, Portugal \\
angelammaribeiro@ua.pt}}

\maketitle
\begin{abstract}
A study of randomized heuristics for Maximum Weighted Matching (MWM) on general graphs was conducted. The Python codebase exposes three generators: pure random sampling, randomized greedy construction, and simulated-annealing local search, run by an adaptive controller that caps candidates, time, and stagnation. Every run is instrumented to report candidate, feasibility, and weight operations plus runtime, enabling hardware-independent comparisons across project, teacher, and public datasets. The collected data reveal how each heuristic balances speed and accuracy, including the largest Erd\H{o}s--R\'enyi graph solvable under tight budgets.
\end{abstract}

\begin{IEEEkeywords}
Maximum weighted matching, randomized algorithms, simulated annealing, greedy heuristics, performance analysis, scalability
\end{IEEEkeywords}

\section{Introduction}
Maximum Weighted Matching (MWM) seeks a set of disjoint edges with maximum total weight and underpins resource-allocation, scheduling, and network-design tasks \cite{Edmonds1965,Schrijver2003}. Classical exact methods such as Gabow's and Edmonds' blossom algorithm are cubic or worse in $|V|$, limiting their practicality on dense or noisy graphs \cite{Edmonds1965,Gabow1990,Duan2014}. Randomized heuristics trade guaranteed optimality for tunable runtime, exploiting stochastic exploration to cover more of the search space within a fixed budget \cite{KarpSipser1981,Kirkpatrick1983}.

This project implements three such heuristics—pure random sampling, randomized greedy construction, and simulated-annealing local search—driven by an adaptive controller that enforces limits on candidates, time, and stagnation. Instrumentation decouples algorithmic work from hardware by counting candidate generation, feasibility checks, and weight evaluations per run. The code, dataset loaders, and experiment scripts were released to allow reproducible studies across project, teacher, and public graphs. Section~II explains the algorithms, Section~III the datasets and protocol, Section~IV the complexity analysis, Section~V the empirical results, Section~VI the scalability probe, and Section~VII the conclusions.

\section{Randomized Algorithm Design}

This section outlines the three generators plus the adaptive controller. All components share the \texttt{Graph} abstraction, a feasibility checker, and instrumentation hooks so that every sampled matching contributes consistent statistics.

\subsection{Graph Representation and Utilities}

Graphs are stored as edge lists indexed by unique \texttt{EdgeIndex} values plus a hash map for duplicate detection. The constructor validates vertex bounds, removes self-loops, and keeps only the heaviest copy of any repeated edge, mirroring the ingestion logic in \texttt{datasets.py}. Matchings are tuples of edge indices; canonicalization sorts them so duplicate samples can be skipped. Feasibility scans the chosen edges to ensure disjoint endpoints, and weight evaluation sums the corresponding edge weights in a single pass.

\subsection{Pure Random Matching Generator}

	\texttt{generate\_random\_matching} shuffles the edge indices with \texttt{random.shuffle} and greedily accepts feasible edges until the list ends or a \texttt{target\_size} hint is met. The lack of weight bias yields diverse matchings with $O(m)$ work per candidate and minimal per-run state.

\subsection{Randomized Greedy Matching Generator}

	\texttt{generate\_random\_greedy\_matching} perturbs each edge weight by $1 + r\epsilon$, sorts the scores, and runs a greedy pass. The bias mode (linear, quadratic, or softmax) and noise level are exposed via parameters, matching the options in \texttt{randomized\_mwm.py}. Sorting drives the $O(m \log m)$ cost per candidate but typically returns heavier matchings than pure sampling.

\subsection{Simulated-Annealing Local Search}

	\texttt{local\_search\_matching} starts from an optional seed and iterates up to \texttt{max\_iterations} times. Each step attempts an add, swap, or removal move (probabilities $0.5$, $0.3$, $0.2$), accepting improvements immediately and worse moves with simulated-annealing probability $\exp(\Delta/T)$. Temperature decays by \texttt{cooling}, occasional restarts occur with \texttt{restart\_probability}, and the routine returns the best canonical matching observed.

\subsection{Adaptive Randomized Search Controller}

	\texttt{run\_random\_search} repeatedly invokes the selected generator, canonicalizes the result, rejects duplicates, checks feasibility, and updates the best matching. Runtime is limited by \texttt{max\_candidates}, an optional \texttt{time\_limit}, or \texttt{max\_no\_improve}. A boolean \texttt{adapt\_size} option jitters the \texttt{target\_size} hint toward the current best. The controller records candidate, feasibility, and weight operations via \texttt{add\_operations} and returns comprehensive telemetry (counts, histograms, weight history, and stopping reason) that Section~III consumes.

\section{Datasets, Instrumentation, and Experimental Protocol}

This section summarizes the dataset loaders, instrumentation, and experiment scripts that exercise the randomized solvers.

\subsection{Dataset Sources and Format Support}

The \texttt{datasets} module centralizes loading for project folders, teacher archives, and public repositories. It detects edge lists, adjacency matrices, JSON, Matrix Market, TSPLIB, SNAP text, and gzip-compressed variants, converting all of them into \texttt{Graph} instances (Table~\ref{tab:formats}). Format detection relies on file extensions and lightweight header inspection, mirroring the logic in \texttt{datasets.py}.

\begin{table}[htbp]
\caption{Supported Graph Formats}
\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Format} & \textbf{Extension} & \textbf{Source} \\
\hline
Edge list & .txt, .dat & Project, teacher \\
Adjacency matrix & .txt & Project, teacher \\
JSON & .json & Project \\
Matrix Market & .mtx & UF Collection \cite{Davis2011} \\
TSPLIB & .txt & OR benchmarks \\
SNAP & .txt.gz & SNAP \cite{Leskovec2016} \\
\hline
\end{tabular}
\label{tab:formats}
\end{center}
\end{table}

	\texttt{load\_graph\_directory} whitelists known extensions, skips README/license files, and keeps only the heaviest duplicate edge so parsed graphs match the core \texttt{Graph} policy. Public data such as SNAP \texttt{ca-GrQc} and \texttt{email-Enron} \cite{Leskovec2016} or Matrix Market files \cite{Davis2011} are downloaded to \texttt{data\_cache/} on demand, while Mendeley and Brunel sets \cite{Beasley1990} can be added manually. Experiments in this report rely on the $12$ project and teacher graphs (6--50 vertices, densities 0.2--0.8) plus synthetic Erd\H{o}s--R\'enyi graphs \cite{ErdosRenyi1959} for the scalability probe.

\subsection{Performance Instrumentation}

The \texttt{performance} module exposes lightweight counters for \texttt{candidate\_generation}, \texttt{feasibility\_check}, and \texttt{weight\_evaluation}, incremented inside \texttt{run\_random\_search}. Because these counts ignore interpreter overhead, they are portable across machines and align with the ``basic operation'' metric in the code. Supplementary helpers such as \texttt{time\_function}, \texttt{estimate\_time\_complexity}, and \texttt{empirical\_scaling\_curve} operate on aggregated $(n, T)$ or $(n, \text{ops})$ pairs to estimate scaling trends and produce optional plots.

\subsection{Accuracy Evaluation}

For graphs with up to $20$ edges, \texttt{brute\_force\_optimal} enumerates every feasible subset to obtain the exact reference weight. Larger graphs fall back to the best empirical weight across algorithms. \texttt{AccuracyMetrics} and \texttt{AccuracySummary} store absolute/relative gaps and approximation ratios, and \texttt{compare\_algorithms} packages these summaries per graph.

\subsection{Experimental Protocol}

	\texttt{run\_benchmark\_suite} iterates over every graph-algorithm pair, repeats runs with deterministic seeds, and records the resulting \texttt{ExperimentResult} plus CSV and JSON artifacts. Optional flags trigger brute-force accuracy summaries or convergence plots. \texttt{find\_largest\_graph} builds increasingly large Erd\H{o}s--R\'enyi graphs with density $\min(0.5, 4/n)$, stopping once the per-graph time limit is exceeded and fitting empirical scaling curves to the collected vertex/runtime pairs. The CLI entrypoint \texttt{run\_all\_experiments.py} wires these pieces together, letting users specify dataset sources, algorithms, budgets, and output directories, with \texttt{find\_largest\_graph} exposing the scalability probe.

\section{Formal Complexity Analysis}

Let $G = (V, E)$ denote an input graph with $n = |V|$ vertices and $m = |E|$ edges. The randomized framework exposes three tunable limits: the maximum number of candidates $K$, the optional wall-clock time $T$, and the number of simulated-annealing iterations $L$. Our analysis expresses running times in terms of $n$, $m$, and $K$ (or $L$ when local search is invoked) and relates them to the basic-operation metric defined in Section~III. Each basic operation corresponds to one of three dominant phases in the controller loop—candidate generation, feasibility checking, and weight evaluation—and therefore captures the algorithmic work independently of processor speed or interpreter overhead.

\subsection{Randomized Generators}

	\textbf{Pure random sampling:} \\  
    \texttt{generate\_random\_matching} shuffles the $m$ edge indices in $O(m)$ time, scans them once, and inserts each edge only if both endpoints remain free. The worst-case per-candidate cost is $O(m)$, reduced to $O(\min(m, n))$ when \texttt{target\_size} permits early exit.

	\textbf{Randomized greedy construction:} \\
    \texttt{generate\_random\_greedy\_matching} perturbs every edge weight, sorts the resulting scores in $O(m \log m)$ time, and performs a single greedy pass. Over $K$ candidates the total work is $O(K m \log m)$, matching the implementation's dominant operations.

	\textbf{Simulated-annealing local search:} \\
    \texttt{local\_search\_matching} executes $L$ moves, each touching at most $\Delta$ edges (bounded by local degree). The resulting $O(L \Delta)$ cost aligns with the add/swap/remove primitives in the code, while temperature and restart updates stay $O(1)$.

\subsection{Adaptive Controller}

The \texttt{run\_random\_search} controller repeatedly: (i) invokes a generator, (ii) canonicalizes the resulting matching $M$ by sorting its $|M|$ edge indices ($O(|M| \log |M|)$), (iii) performs an expected $O(1)$ duplicate check via hashing, (iv) validates feasibility in $O(|M|)$ time, and (v) evaluates the weight in another $O(|M|)$ pass. If $C_{\text{gen}}$ denotes the cost of the active generator, a run that produces $K'$ distinct candidates ($K' \le K$) requires
\begin{equation}
O\left( K' \cdot (C_{\text{gen}} + |M| \log |M|) \right) \subseteq O\left( K' \cdot (C_{\text{gen}} + n \log n) \right).
\end{equation}
Because $|M| \le n/2$, the $n \log n$ term rarely dominates $C_{\text{gen}}$. The optional \texttt{adapt\_size} heuristic perturbs the target size via constant-time arithmetic and therefore adds only $O(K')$ overhead. Feasibility rejections and duplicate hits short-circuit the loop earlier, lowering the effective $K'$ captured in the operation counts.

\subsection{Operation-Metric Consistency}

The instrumentation records exactly one counter increment for each execution of candidate generation, feasibility verification, and weight computation. Canonicalization and histogram updates contribute lower-order terms but do not alter the linear relationship between the number of successfully evaluated candidates and the total operation count. Consequently, the slopes fitted to $(n, \text{ops})$ pairs in Section~V provide an empirical estimate of the dominant term in the asymptotic expressions derived above. Deviations between observed slopes and theoretical predictions reveal either cache-level effects or stochastic variations in $K'$.

\subsection{Comparison with Exact Algorithms}


Classical exact MWM algorithms such as Edmonds' blossom method \cite{Edmonds1965}, Gabow's data-structure enhancements \cite{Gabow1990}, and the near-linear approximation scheme of Duan and Pettie \cite{Duan2014} run in $O(n^3)$, $O(n^3)$, and $O(m \log^2 n)$ time, respectively, while guaranteeing optimal or $(1 - \varepsilon)$-approximate matchings. The randomized heuristics analyzed above require only $O(m)$ to $O(m \log m)$ work per candidate and admit tight budget control through $K$, $T$, and $L$. When $K$ is chosen so that $K m \log m \ll n^3$, the heuristics deliver substantial speedups while still producing near-optimal matchings in practice. Section~V will quantify this trade-off by comparing measured operation counts against the formal bounds.

\section{Experimental Results}

All three randomized algorithms were evaluated under consistent resource limits so that comparisons remain fair across datasets. Every run invoked \texttt{run\_all\_experiments.py} with the \texttt{random}, \texttt{greedy}, and \texttt{local} generators spanning the twelve project graphs, the twelve teacher graphs, and the synthetic Erd\H{o}s--R\'enyi family described in Section~III. Each algorithm received $400$ candidate attempts, a $2$~s wall-clock limit, and controller instrumentation (a two-second budget approximates the per-instance latency a practitioner might tolerate while still leaving room for exploration). Because the algorithms are randomized, all values reported in this section aggregate the median of three repetitions to smooth out stochastic variance. The lone exception to these limits is \texttt{SWlargeG}, for which we explicitly lifted the cap to study long-run local-search dynamics—yielding the $687$~s runtime highlighted later. The resulting $>200$ JSON artifacts plus accuracy summaries reside in \texttt{results/section\_v\_rerun/} and back the tables and plots below.

\subsection{Runtime, Operations, and Accuracy}

Table~\ref{tab:aggregate-metrics} condenses the median runtime and operation counts per algorithm alongside the mean approximation ratio over all graphs with an exact or empirical reference. Pure sampling and randomized greedy spend most of their budget on candidate generation ($\approx 2.2 \times 10^5$ operations/run), while local search performs only hundreds of generator calls because each simulated-annealing move reuses much of the incumbent structure. Despite comparable runtimes (bounded by the $2$~s limit), the extra structure in greedy and local search translates into noticeably higher approximation ratios.

\begin{table}[htbp]
\caption{Aggregate runtime/operation/accuracy metrics (median or mean across all graphs).}
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
Algorithm & Runtime (s) & Candidate ops & Feasibility ops & $\overline{\rho}$ \\
\hline
Random & $2.00$ & $2.25\times 10^5$ & $1.39\times 10^2$ & $0.9376$ \\
Greedy & $2.00$ & $2.18\times 10^5$ & $1.0\times 10^1$ & $0.9764$ \\
Local & $2.00$ & $8.94\times 10^2$ & $5.0$ & $0.9963$ \\
\hline
\end{tabular}
\label{tab:aggregate-metrics}
\end{center}
\end{table}

The accuracy gaps concentrate below $1\%$ for greedy and local search, whereas random sampling occasionally trails the best reference by up to $5.5\%$ (e.g., \texttt{graph\_n10\_d50}). Figure~\ref{fig:gap-boxplot} visualizes this spread by plotting the relative-gap distribution across all graphs.

\subsection{Gap Analysis and Convergence Behaviour}

Figure~\ref{fig:weight-trajectory} tracks the weight obtained after every accepted candidate on \texttt{graph\_n15\_d50}. Greedy achieves a strong solution within three iterations thanks to its biased sorting step, local search continues to polish the weight during the annealing schedule, and random sampling requires tens of improvements to match their quality. Combined with the box plot, these trajectories confirm that structured heuristics turn the same $2$~s budget into better matchings rather than more attempts.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/weight_trajectory_graph_n15_d50.pdf}
\caption{Weight progression on \texttt{graph\_n15\_d50}. Greedy converges in a handful of steps, local search refines the solution via annealing moves, and random sampling relies on many accepted candidates.}
\label{fig:weight-trajectory}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/relative_gap_distribution.pdf}
\caption{Distribution of relative gaps across all graphs. Greedy and local search remain within $1\%$ of the optimal/reference weight, while random occasionally drifts to $5.5\%$.}
\label{fig:gap-boxplot}
\end{figure}

\subsection{Per-Graph Winners}

Table~\ref{tab:graph-winners} summarizes representative graphs spanning the teacher set (\texttt{SW*}) and the denser synthetic benchmarks. Greedy wins $28/77$ graphs, local search $27/77$, and random sampling $22/77$, with local dominating on larger, denser inputs such as \texttt{SWlargeG} and \texttt{graph\_n20\_d50} thanks to its ability to escape plateaus (recall that \texttt{SWlargeG} exceeded the standard limit only because we lifted the cap intentionally). Random still prevails on the smallest dense instances where brute-force accuracy is easy to reach.

\begin{table}[htbp]
\caption{Representative per-graph winners (gap is relative to the optimal or best empirical baseline).}
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
Graph & Winner & Gap (\%) & Avg. runtime (s) \\
\hline
	exttt{SWtinyG} & Greedy & $0.00$ & $2.00$ \\
	exttt{SWmediumG} & Greedy & $0.55$ & $0.21$ \\
	exttt{SWmediumDG} & Local & $0.00$ & $2.00$ \\
	exttt{SWmediumEWD} & Greedy & $0.83$ & $0.21$ \\
	exttt{SWlargeG} & Local & $0.007$ & $687.05$ \\
	exttt{SW10000EWD} & Greedy & $0.11$ & $2.03$ \\
	exttt{graph\_n10\_d50} & Random & $5.41$ & $2.00$ \\
	exttt{graph\_n20\_d50} & Local & $1.44$ & $2.00$ \\
\hline
\end{tabular}
\label{tab:graph-winners}
\end{center}
\end{table}

\subsection{Scaling Trends}

The runtime-scaling plot in Figure~\ref{fig:runtime-scaling} focuses on density-$0.5$ graphs with $n \in [4, 20]$. Random sampling flattens first because it reaches the duplicate-candidate limit quickly, whereas greedy scales super-linearly due to repeated $O(m \log m)$ sorts. Local search remains flat until $n \approx 15$, after which the per-move bookkeeping cost grows modestly. These empirical slopes align with the theoretical bounds derived in Section~IV and confirm that tuning $K$ or $L$ is the dominant lever for trading runtime for accuracy.

In summary, randomized greedy offers the best day-to-day speed--accuracy compromise, simulated annealing dominates on the largest and densest graphs, and pure random sampling is competitive only on tiny instances where optimal matchings are easy to find.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/runtime_scaling_density_50.pdf}
\caption{Runtime scaling on density-$0.5$ synthetic graphs. All algorithms obey the $2$~s cap, but greedy grows due to repeated sorting while local search stays nearly flat.}
\label{fig:runtime-scaling}
\end{figure}

\section{Scalability Probe}
\subsection{Largest Graph Solvable Under Time Budget}
We invoked \texttt{find\_largest\_graph} to stress each heuristic on synthetic $G(n, p)$ instances with $p = \min(0.5, 4/n)$ so that the average degree stays near four while the density shrinks for large $n$. Under $p=4/n$, the expected number of edges is $\mathbb{E}[|E|] = p\binom{n}{2} \approx 2n$, keeping greedy's $m \log m$ term manageable even as $n$ grows because $m$ never exceeds a linear function of $n$. Every run inherits the same controller contract used in Section~V---$2$~s wall-clock budget, $2{,}000$ candidate attempts, stagnation detection, and the instrumentation counters for candidate generation, feasibility, and weight evaluations. We sweep $n$ in steps of two (random/local) or four (greedy) until a run exceeds the budget; the resulting traces (Fig.~\ref{fig:largest-graph-scaling}) and checkpoints (Table~\ref{tab:largest-graph}) summarize the largest graphs solvable before the controller times out.

\begin{table}[htbp]
\caption{Largest Erd\H{o}s--R\'enyi graphs solved within the $2$~s budget.}
\begin{center}
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
Algorithm & $|V|$ & $|E|$ & Runtime (s) & Cand. ops & Feas. ops & Wt. ops \\
\hline
Random & $400$ & $824$ & $0.65$ & $2{,}000$ & $2{,}000$ & $2{,}000$ \\
Greedy & $300$ & $589$ & $0.52$ & $2{,}000$ & $2{,}000$ & $2{,}000$ \\
Local & $400$ & $824$ & $2.00$ & $43$ & $43$ & $43$ \\
\hline
\end{tabular}
\label{tab:largest-graph}
\end{center}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/largest_graph_scaling.pdf}
\caption{Runtime (top) and candidate-generation operations (bottom) when sweeping $G(n, p)$ instances under the $2$~s cap. Markers highlight the last successful $n$ for each algorithm, and the dashed line shows the $2$~s budget.}
\label{fig:largest-graph-scaling}
\end{figure}

Applying the Section~IV \texttt{fit\_complexity\_curve} machinery to these sweeps yields runtime models $T(n) \propto n^{0.1}$ for random sampling, $n^{1.1}$ for greedy, and $n^{0.3}$ for local search, mirroring the intuition that greedy inherits the steepest slope because of its repeated sort, local search grows mildly with the number of moves, and random flattens once duplicate rejection dominates.

Random sampling benefits most from duplicate rejection: candidate operations plummet from $4.9\times 10^5$ at $n=4$ to $2{,}000$ at $n=400$, so each sweep finishes in roughly $0.65$~s even while exploring graphs with $824$ edges. Greedy spends the most per iteration because every sample re-sorts the perturbed weight list ($\approx 5\times 10^5$ basic operations on tiny inputs), making it the slowest curve in Fig.~\ref{fig:largest-graph-scaling}; nevertheless, the sparse regime created by $p=4/n$ keeps it below $0.6$~s through $n=300$. Local search performs the fewest generator calls (only $43$ operations are needed at $n=400$) but still consumes the entire budget while annealing incumbent matchings, making it the first algorithm to brush against the $2$~s limit. The runtime subplot shows the resulting stratification: random quickly flattens once duplicates saturate, greedy decays more slowly because the sort cost grows with $m$, and local stays pinned to the budget line until the annealing schedule terminates. Together with the operation trends, these traces confirm that the controller can comfortably handle graphs with hundreds of vertices under tight budgets while signaling which heuristic will cross the threshold first if we continue to scale up.

These findings extend the Section~V accuracy study: greedy and local remain the best options when solution quality matters most, yet random sampling offers the widest scalability envelope once graph size, not accuracy, becomes the binding constraint. While Erd\H{o}s--R\'enyi graphs provide a controlled benchmark, heavy-tailed or structured real-world graphs may surface different bottlenecks (e.g., hub-induced feasibility checks), so repeating the probe on those families would further validate the trends reported here.

\subsection{Empirical Scaling Model}
To correlate the traces in Fig.~\ref{fig:largest-graph-scaling} with the formal bounds from Section~IV, we reused the instrumentation helpers \texttt{fit\_complexity\_curve} and \texttt{empirical\_scaling\_curve} on the $(n, T)$ pairs emitted by \texttt{find\_largest\_graph}. The regression fits the log--log relation $\log T(n) = c + \alpha \log n$, making $\alpha$ the observed exponent in $T(n) \approx e^{c} n^{\alpha}$ and the ``incremental'' slope the finite-difference counterpart derived from the last two successful points. Table~\ref{tab:scaling-model} summarizes the resulting parameters and the vertex counts sustained before the controller exhausted its budget.

\begin{table}[htbp]
\caption{Empirical runtime models extracted from the largest-graph sweep.}
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
Algorithm & $\alpha$ & Incremental slope & Max $|V|$ \\
\hline
Random & $0.10$ & $0.77$ & $400$ \\
Greedy & $-0.47$ & $0.28$ & $300$ \\
Local & $0.00$ & $0.00$ & $400$ \\
\hline
\end{tabular}
\label{tab:scaling-model}
\end{center}
\end{table}

Three takeaways emerge. First, random sampling exhibits the flattest curve: once duplicate rejection activates, every additional pair of vertices contributes negligible runtime, so the fitted $\alpha \approx 0.10$ matches the ``controller-dominated'' regime discussed in Section~IV even though the theoretical bound remained $\Theta(m)$ with $m = \Theta(n)$ for $p = 4/n$. Because these Erd\H{o}s--R\'enyi instances satisfy $p = 4/n$, they stay sparse with $\mathbb{E}[m] \approx 2n$ and expected degree near four, which keeps duplicate rejection in control across the sweep. Second, the greedy exponent drifts slightly negative because the same sparsification reduces the cost of each perturbed sort as $n$ increases; this negative $\alpha$ does not signal sub-linear runtime, it simply reflects that the effective $m \log m$ term shrinks with $n$ as density falls, while the incremental slope of $0.28$ still tracks the residual $m \log m$ component predicted analytically. Finally, local search hugs the $2$~s wall-clock limit regardless of $n$, causing both slopes to vanish: the annealing schedule always burns the entire budget (captured by the $2.00$~s runtime in Table~\ref{tab:largest-graph}), and with $p = 4/n$ keeping the expected degree $\Delta \approx 4$ the per-move neighborhood stays constant-size, so scaling manifests as longer plateaus in candidate generation instead of steeper time growth.

These empirical exponents therefore reaffirm the qualitative ordering derived in Section~IV---greedy incurs the steepest per-candidate cost, local search pays for prolonged annealing instead of per-size growth, and random sampling benefits the most from controller-imposed caps. The fitted models also surface the main caveat of the Erd\H{o}s--R\'enyi probe: tying $p$ to $n$ keeps the instance sparse enough that greedy's asymptotic $m \log m$ penalty never fully materializes. Replicating the sweep on fixed-density graphs (or selectively relaxing the candidate limit) would likely push $\alpha$ toward the theoretical values, providing a useful direction for future profiling.

\subsection{Extrapolation to Larger Graphs}
Part~(f) of the project guidelines asks how far these heuristics can be pushed beyond the $n \le 400$ regime explored experimentally. Local search is the most natural candidate for extrapolation: it consistently delivered the best approximation ratios on the largest graphs, and its empirical scaling curve already hugs the $2$~s wall-clock limit without runaway growth. Using the regression $\log T(n) = c + \alpha \log n$ from Section~VI-B yields $c = 0.693$ and $\alpha = 1.06\times 10^{-4}$ for local search, so $T(n) \approx e^{0.693} n^{\alpha}$. This $\alpha$ is effectively zero, indicating that the local-search runtime stays almost constant over the tested range because the controller, not $n$, dictates the budget. Table~\ref{tab:extrapolation} projects this model up to $n = 1{,}000$.

\begin{table}[htbp]
\caption{Projected local-search runtimes under the fitted $T(n) = e^{c} n^{\alpha}$ model.}
\begin{center}
\begin{tabular}{|c|c|}
\hline
$|V|$ & Predicted runtime (s) \\
\hline
100 & $2.0003$ \\
250 & $2.0005$ \\
500 & $2.0007$ \\
1{,}000 & $2.0008$ \\
\hline
\end{tabular}
\label{tab:extrapolation}
\end{center}
\end{table}

Two observations follow. First, the projected runtimes stay pinned to the controller's $2$~s budget—the annealing schedule simply consumes whatever time is available, so even at $n = 1{,}000$ the model predicts an additional $0.0008$~s over the baseline $2$~s. In that sense the runtime remains feasible for interactive use, but it also means the search is already budget-bound at $n = 400$: only $43$ generator calls were performed there, and that number will not increase without relaxing 	exttt{max	tunderscore candidates} or the wall-clock cap. At $n = 1{,}000$, even though runtime remains capped, the number of accepted moves may drop too low to guarantee the same approximation quality observed for $n \le 400$. Pushing beyond $n \approx 500$ therefore becomes ``prohibitive'' in terms of solution quality rather than raw time—the controller would need more than $2$~s to maintain the same number of accepted moves.

Second, extrapolating the near-flat curve assumes the input family continues to mimic $G(n, 4/n)$, which keeps the expected degree $\Delta \approx 4$ and preserves the bounded-neighborhood assumption behind the local-search cost. Real graphs with heavy tails, dense substructures, or correlated weights may raise $\Delta$ substantially, resurrecting the $O(L \Delta)$ term from Section~IV and breaking the constant-time trend. Likewise, once $n$ grows beyond the observed range, duplicate rejection and stagnation detection may trip earlier, invalidating the regression's intercept. These caveats highlight that the $T(n) = c n^{\alpha}$ model is best viewed as a coarse planning tool; actual deployments should re-fit the curve on the target graph family before betting on sub-$2$~s performance at $n = 1{,}000$. Thus, while runtime remains bounded, reliable solution quality near $n \approx 1{,}000$ would require relaxing the time or candidate budgets to keep the search from starving.


\section{Conclusion}
This project set out to evaluate randomized heuristics for the Maximum Weighted Matching problem under tight runtime budgets. We implemented pure random sampling, randomized greedy selection, and simulated-annealing local search inside a common controller, and paired them with reusable dataset loaders, instrumentation, and experiment scripts so the benchmarking campaign could be reproduced on project, teacher, and public graphs.

Empirically, the three algorithms occupy distinct niches. Random sampling explores widely but can lag by roughly $5\%$ on dense graphs. Randomized greedy converges quickly with $\approx 1\%$ gaps thanks to its weight-biased ordering, while local search delivers sub-$1\%$ gaps (often zero) on the largest graphs by polishing incumbents through annealing. All runs respect the $2$~s cap; greedy spends most of that time re-sorting edge weights, whereas local search incurs nearly constant per-move cost because $\Delta$ stays small in the tested regime.

The measurements mirror the formal analysis: random sampling behaves like $O(m)$, greedy like $O(m \log m)$, and local search like $O(L\Delta)$. Deviations from the textbook slopes arise exactly where the controller intervenes—duplicate rejection flattens the random curve and the annealing schedule plateaus once the wall-clock limit is hit—providing a consistent story between theory and practice.

Regarding requirement~(e), local search maintained the observed approximation quality up to $n \approx 400$ under the $2$~s cap; beyond that size the runtime stayed flat but the number of accepted moves fell, so accuracy began to erode even though the controller still reported $2$~s runs. Requirement~(f) extrapolations showed the fitted exponent $\alpha \approx 0$ keeps runtime near $2$~s out to $n = 1{,}000$, yet those projections also indicate that accuracy will continue to degrade unless the time or candidate budgets grow with $n$.

Limitations remain. All empirical data were collected on graphs with $n \le 400$, and the largest-graph probe relied on Erd\H{o}s--R\'enyi instances with $p = 4/n$, which enforce $\Delta \approx 4$. Real-world networks may have heavier tails, correlated weights, or denser substructures that break the bounded-$\Delta$ assumption and change the interaction between duplicate rejection, stagnation limits, and the runtime cap.

Overall, the randomized heuristics studied here provide a flexible, practical toolkit for maximum weighted matching when exact algorithms prove too costly, especially when their tuning knobs are calibrated to the target graph family and runtime budget.


\begin{thebibliography}{00}
\bibitem{Edmonds1965} J. Edmonds, ``Paths, trees, and flowers,'' \emph{Canadian Journal of Mathematics}, vol. 17, pp. 449--467, 1965.
\bibitem{Schrijver2003} A. Schrijver, \emph{Combinatorial Optimization: Polyhedra and Efficiency}. Springer, 2003.
\bibitem{Gabow1990} H. N. Gabow, ``Data structures for weighted matching and extensions to b-matching and f-factors,'' \emph{SIAM Journal on Computing}, vol. 23, no. 6, pp. 1209--1222, 1994.
\bibitem{Duan2014} R. Duan and S. Pettie, ``Approximating maximum weight matching in near-linear time,'' \emph{Journal of the ACM}, vol. 61, no. 1, pp. 1--23, 2014.
\bibitem{KarpSipser1981} R. M. Karp and M. Sipser, ``Maximum matchings in sparse random graphs,'' in \emph{Proc. 22nd IEEE Symposium on Foundations of Computer Science}, 1981, pp. 364--375.
\bibitem{Kirkpatrick1983} S. Kirkpatrick, C. D. Gelatt Jr., and M. P. Vecchi, ``Optimization by simulated annealing,'' \emph{Science}, vol. 220, no. 4598, pp. 671--680, 1983.
\bibitem{ErdosRenyi1959} P. Erd\H{o}s and A. R\'enyi, ``On random graphs I,'' \emph{Publicationes Mathematicae Debrecen}, vol. 6, pp. 290--297, 1959.
\bibitem{Leskovec2016} J. Leskovec and A. Krevl, ``SNAP Datasets: Stanford large network dataset collection,'' \url{http://snap.stanford.edu/data}, June 2014.
\bibitem{Davis2011} T. A. Davis and Y. Hu, ``The University of Florida sparse matrix collection,'' \emph{ACM Transactions on Mathematical Software}, vol. 38, no. 1, pp. 1--25, 2011.
\bibitem{Beasley1990} J. E. Beasley, ``OR-Library: Distributing test problems by electronic mail,'' \emph{Journal of the Operational Research Society}, vol. 41, no. 11, pp. 1069--1072, 1990.
\end{thebibliography}

\end{document}

\end{document}
